# Устройство кластера

## Компоненты

Все компоненты - бинарники (по сути - это поды внутри кластера, работают в контейнерах). Есть мастер нода (оганичение на запуск подов, хотя можно обойти) и обычные. 

### Etcd 
Master node. Key-value БД кластера. Кластерная БД, работает на протоколе raft (!!!ссылка - secret life of data)
'''(!!!Схема)'''
3-5 нод с etcd. 

### Api Server
Master node. Центральный компонент кластера. Работает по REST-API. kubectl - работает через него. 
Функции: 
- Все взаимодействие внтури и вне через его, только он работает с БД. 
- Обычно несколько в кластере. 
- AAA

### Controller-manager
Master node. Один файл, но много контроллераов внутри. 
Запущено обычно несколько, но работает 1. Рабочий определяется по механизму lease. 
Отслеживает события и создает связанные сущности. Отслеживание - механизм http2 (long polling, watch)

- Node controller - проверка живости нод (пишут в node controller), 5 мин. В случае недоступности - вызывает Api Server. 
- ReplicaSetController - создает поды из описания ReplicaSet
- EndpointController
...
__GarbageColllector__ - удаляет ненужные сущности (более старые чем задано в настройках)

### Sheduler
Master node.  Назначает поды на ноды. Не создает, а только выбирает. 
Запущено обычно несколько, но работает 1. Рабочий определяется по механизму lis. 

**Учитывает**:
- QoS
- Affinity / anti-affinity (указание на запуск подов на определенной ноде, например одного деплоймента или наоборот, все поды на разных)
- Requested resources (исходя из имеющихся ресурсов на нодах)
- Priority class (обозначаем важность приложений)

**Алгорим**: фильтруем не подходящие - скорим оставшиеся - выбираем нужную

### Kubelet
Работает на всех нодах кластера (в т.ч. на мастер). Единснтвенный, кто работает не в контейнере (процесс на хосте), наоброт отправляет команды демону контейнеризации (напрмиер докер). 
Функции:
- реально создает поды
- делает пробы (liveness, readyness, startup) - вызывается по http с той же ноды (по сути по localhost)
- отдает информацию по ноде: живость, контейнеры, ресурсы

Только после выполнения команды kubelet запускается реальное приложение

Есть собственный garbage collector (при потреблении диска на ноде 90%). 

Внутри кластера никто не отдает команды, каждый компонент приводит свою часть инфрастуктры -> сообщает api server и получает информацию от других компонент (так же через api server)

### Kube-proxy
Стоит на всех нодах, смотри в api server. Живет в контейнере.
Функция - управление сетевыми правилами на ноде. Реализует правила, заданные в сервисах (по сути, конфигурирует iptables на ноде, реально не проксирует трафик)

Не поговорим про: Контейнеризация, Сеть, DNS. Без них kubernetes может запустить и их реализация не регламентирована (docker/crio, current - calica/civil/kuberouter, current - core-dns)

### Зачем оно мне?

- Кейс - запустили деплоймент, он появится, но подов нет. Решение: смотрим репликасеты - нет -> возможно помомался controller-manager. 
Если бы была проблема с шедулером - были бы поды, но не распределены на ноду (статус pending)
- Кейс - очень медленно создаются сущности. Решение: тупит api server или etcd, т.к. все общение проходит через них, но при этом сущности создаются - значит остальные компоненты работают хорошо. 

### QA
1) Как происходит запуск кластера?
Запускают админы или автоматические (managed). Часто используются средства автоматизации - ansible-скрипты, chief, puppet, или специализированные kubeadm. 
2) Как здобавить ноду?
Добавить kubelet на ноду, настроить ее взаимодействие с api server по htpps. 
3) Где запустить БД?
Лучше всего на другом физическом или виртуальном сервере, но теоретически при многих допущениях можно и в kubernetes. 
4) Нужны ли высокопроизводительный диски для etcd? Да, как минимум SSD для всех реплик. 
5) Откуда берутся поды в кластере (ведь все компоненты  - это поды), если сам кластер kubernetes еще не запущен?
Есть статические поды, если в определенной директории на нодах разместить манифесты (описания) подов, то kebelet при запуске их создат. Поэтому сначала запускают kubelet на мастер ноде -> он полнимает все связанные компоненты (статические поды) -> потому уже добавляются остальные ноды.


!!! Посмотреть в чате ссылки на watcher, listerner etc.